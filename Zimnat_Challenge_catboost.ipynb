{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Zimnat_Challenge_catboost.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSqCv4yFHdIr"
      },
      "source": [
        "Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qY5eLdprRfjF"
      },
      "source": [
        "!pip install catboost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4FzmErNfkix"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from catboost import CatBoostClassifier, Pool, cv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score, log_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38mSXjW1gZoh"
      },
      "source": [
        "Read the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v91gF2tBgCs-"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3-1-nuusB1Q"
      },
      "source": [
        "import io\n",
        "df_test = pd.read_csv(io.BytesIO(uploaded['Test.csv']))\n",
        "df_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtF4jseMftXo"
      },
      "source": [
        "df_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZKStaav9r2z"
      },
      "source": [
        "Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XXtNyIPtAuc"
      },
      "source": [
        "df_train['join_date'] = pd.to_datetime(df_train['join_date'], format=\"%d/%m/%Y\")\n",
        "df_train['join_days'] = (pd.to_datetime('1/8/2020', format=\"%d/%m/%Y\")- df_train['join_date']).dt.days\n",
        "df_train['age'] = 2020 - df_train['birth_year']\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v55Vgq9XnpHn"
      },
      "source": [
        "Normalisation of Join_days and age"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHvzmqNx1hDn"
      },
      "source": [
        "scaler = MinMaxScaler()\n",
        "cols_to_norm = ['join_days','age']\n",
        "df_train[cols_to_norm] = scaler.fit_transform(df_train[cols_to_norm])\n",
        "df_train['sex'].replace('F', 0, inplace=True)\n",
        "df_train['sex'].replace('M', 1, inplace=True)\n",
        "df_train_scaled = df_train.drop(['ID', 'join_date', 'birth_year'], axis=1)\n",
        "df_train_scaled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-h-RPDpM8bD"
      },
      "source": [
        "Test Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ToRWInVLcWR"
      },
      "source": [
        "df_test['join_date'] = pd.to_datetime(df_test['join_date'], format=\"%d/%m/%Y\")\n",
        "df_test['join_days'] = (pd.to_datetime('1/8/2020', format=\"%d/%m/%Y\")- df_test['join_date']).dt.days\n",
        "df_test['age'] = 2020 - df_test['birth_year']\n",
        "df_test[cols_to_norm] = scaler.transform(df_test[cols_to_norm])\n",
        "df_test['sex'].replace('F', 0, inplace=True)\n",
        "df_test['sex'].replace('M', 1, inplace=True)\n",
        "df_test_scaled = df_test.drop(['ID', 'join_date', 'birth_year'], axis=1)\n",
        "df_test_scaled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN_o7reTqzNE"
      },
      "source": [
        "One Hot encoding of categorical features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFRZTydqKshU"
      },
      "source": [
        "df_train_scaled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuaJTqQNVsqZ"
      },
      "source": [
        "df_train_scaled[\"join_days\"]= df_train_scaled[\"join_days\"].fillna(0)\n",
        "df_test_scaled[\"join_days\"]= df_test_scaled[\"join_days\"].fillna(0)\n",
        "df_test_scaled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbkNoiZo16Gx"
      },
      "source": [
        "Function to predict all products"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf3WifkNx4eX"
      },
      "source": [
        "def predict_all_products(train_data, test_data, products_list):  \n",
        "  predicted_data = pd.DataFrame(columns = products_list)\n",
        "  predicted_proba = pd.DataFrame()\n",
        "  f1_scores = []\n",
        "  models = {}\n",
        "  #accuracy_score(y_test, y_pred)\n",
        "  for product in products_list:\n",
        "    target_col = product\n",
        "    X = train_data.loc[:,train_data.columns != target_col]\n",
        "    y = train_data.loc[:, target_col]\n",
        "    X_eval = test_data.loc[:, test_data.columns != target_col]\n",
        "    y_eval = test_data.loc[:, target_col]    \n",
        "    X_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.75)\n",
        "    \n",
        "    cat_features = ['marital_status', 'branch_code', 'occupation_code', 'occupation_category_code']\n",
        "\n",
        "    # Initialize CatBoostClassifier\n",
        "    model = CatBoostClassifier(eval_metric=\"Logloss\") #, learning_rate=0.2,task_type='GPU',\n",
        "\n",
        "    # Fit model\n",
        "    model.fit(X_train, y_train, cat_features= cat_features, eval_set=(X_validation, y_validation), plot=True) #eval_set=(X_eval, y_eval)\n",
        "    #f1_scores.append(f1_score(y_test, y_pred))\n",
        "    y_eval_pred = model.predict(X_eval)\n",
        "    \n",
        "    models[product] = model\n",
        "    #prob_0=product+'_0'\n",
        "    #prob_1=product+'_1'\n",
        "    y_eval_pred_proba = model.predict_proba(X_eval)\n",
        "    predicted_data[target_col] = y_eval_pred\n",
        "    # Probabilities of product 0 and 1    \n",
        "    #predicted_proba[prob_0] = y_eval_pred_proba[:,0]\n",
        "    predicted_proba[product] = y_eval_pred_proba[:,1]\n",
        "    \n",
        "  return models, predicted_data, predicted_proba\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtLiV_Yi6jGv"
      },
      "source": [
        "products = ['P5DA', 'RIBP', '8NN1', '7POT',\t'66FJ',\t'GYSR', 'SOP4', 'RVSZ', 'PYUQ', 'LJR9', 'N2MW', 'AHXO', 'BSTQ', 'FM3X', 'K6QO', 'QBOL', 'JWFN', 'JZ9D', 'J9JW',\t'GHYX',\t'ECY3']\n",
        "prediction_data = []\n",
        "models, prediction_data, prediction_proba = predict_all_products(df_train_scaled, df_test_scaled, products)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioFkOvL22BBy"
      },
      "source": [
        "Function to predict a single product\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAAdD43-2GaB"
      },
      "source": [
        "# global variables\n",
        "predicted_prod_data = pd.DataFrame()\n",
        "predicted_prod_proba = pd.DataFrame()\n",
        "def predict_single_product(all_data, train_data, test_data, product):  \n",
        "  \n",
        "  \n",
        "  all_data[['sex', 'P5DA', 'RIBP', '8NN1', '7POT',\t'66FJ',\t'GYSR', 'SOP4', 'RVSZ',\n",
        "              'PYUQ', 'LJR9', 'N2MW', 'AHXO', 'BSTQ', 'FM3X', 'K6QO', 'QBOL', 'JWFN', 'JZ9D',\n",
        "              'J9JW',\t'GHYX',\t'ECY3', 'Weight']] = all_data[['sex', 'P5DA', 'RIBP', '8NN1', '7POT',\t'66FJ',\n",
        "                                                               'GYSR', 'SOP4', 'RVSZ', 'PYUQ', 'LJR9', 'N2MW', 'AHXO', 'BSTQ',\n",
        "                                                               'FM3X', 'K6QO', 'QBOL', 'JWFN', 'JZ9D', 'J9JW',\t'GHYX',\t'ECY3',\n",
        "                                                               'Weight']].apply(pd.to_numeric)\n",
        "\n",
        "  \n",
        "  target_col = product\n",
        "  # Training data\n",
        "  weights = 'Weight'\n",
        "  weight = all_data.loc[:, weights].tolist()\n",
        "  del all_data[weights]\n",
        "  #removal_data =product.append(weight)\n",
        "  \n",
        "  X_eval = all_data.loc[:,all_data.columns != target_col]   \n",
        "  #X_train = X_train.loc[:,X_train.columns != weights]\n",
        "  y_eval = all_data.loc[:, target_col]\n",
        "  y_eval.astype(float)  \n",
        "  # Test data\n",
        "  X_test = test_data.loc[:, test_data.columns != target_col]\n",
        "  y_test = test_data.loc[:, target_col]\n",
        "  #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)  \n",
        "  cat_features = ['marital_status', 'branch_code', 'occupation_code', 'occupation_category_code']\n",
        "  \n",
        "  eval_data = Pool(data= X_eval, label=y_eval, cat_features= cat_features, weight=weight)\n",
        "\n",
        "  X_train = train_data.loc[:,train_data.columns != target_col]\n",
        "  y_train = train_data.loc[:, target_col]  \n",
        "\n",
        "  model = CatBoostClassifier(iterations=1600, eval_metric=\"Logloss\") #, learning_rate=0.2,task_type='GPU',\n",
        "\n",
        "  # Fit model\n",
        "  model.fit(X_train, y_train, eval_set=eval_data, cat_features= cat_features, plot=True) \n",
        "  \n",
        "  y_eval_pred = model.predict(X_test)   \n",
        "  y_eval_pred_proba = model.predict_proba(X_test)\n",
        "  predicted_prod_data[target_col] = y_eval_pred\n",
        "  #df = pd.DataFrame(data=y_eval_pred_proba, columns=[prob_0, prob_1])\n",
        "  #predicted_proba[prob_0] = y_eval_pred_proba[:,0]\n",
        "  predicted_prod_proba[product] = y_eval_pred_proba[:,1]\n",
        "  # Calculate Loss\n",
        "  #loss = log_loss(y_eval, y_eval_pred_proba, labels=[0,1])\n",
        "  #predicted_proba.join(df)\n",
        "  return #, loss\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cuo4Cqx5SiE7"
      },
      "source": [
        "# For each product, calculating the probabilities\n",
        "df_test_expanded=pd.DataFrame()\n",
        "#predicted_prod_data=pd.DataFrame()\n",
        "#predicted_prod_proba=pd.DataFrame()\n",
        "for product in products:\n",
        "  # Filtering the data for every column (product) = 1\n",
        "  df_product = df_test_scaled.loc[df_test_scaled[product] == 1]  \n",
        "  remaining_products = products.copy()\n",
        "  remaining_products.remove(product)\n",
        "  checked_products = [product]\n",
        "  # Dataframe for each product with new derived data to run prediction  \n",
        "  df_product_new_data = pd.DataFrame(columns=df_test_scaled.columns)  \n",
        "  # adding new column\n",
        "  df_product_new_data['Weight'] = 0    \n",
        "  product_new_data_list = []                                                     \n",
        "  #print(df_product)\n",
        "  first_run = True\n",
        "  for index, row in df_product.iterrows():\n",
        "    for indiv_product in remaining_products:      \n",
        "      # Creation of new row only when the product value is 0\n",
        "      if(row[indiv_product] != 1):        \n",
        "        # Update weight with probability\n",
        "        weight = prediction_proba.loc[index, indiv_product]  #[indiv_product][index]\n",
        "        #temp.append(weight)\n",
        "        # Create new rows for product with weights        \n",
        "        product_new_data_row_dict= row.to_dict()\n",
        "        product_new_data_row_dict['Weight'] = weight\n",
        "        product_new_data_row_dict[indiv_product] = 1\n",
        "        product_new_data_list.append(product_new_data_row_dict.values())\n",
        "  df_product_new_data = pd.DataFrame(product_new_data_list, columns = df_product_new_data.columns)                \n",
        "  \n",
        "  predict_single_product(df_product_new_data, df_train_scaled, df_test_scaled, product)\n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VN0gCrZK5jYR"
      },
      "source": [
        "# Count the number of misclassification\n",
        "# wrong_pred_count=0\n",
        "# for product in products:\n",
        "#   for row in range(len(predicted_prod_data.index)):\n",
        "#     if predicted_prod_data[product][row] != df_test_scaled[product][row] and df_test_scaled[product][row] == 1:\n",
        "#       print(product,row)\n",
        "#       wrong_pred_count += 1      \n",
        "# wrong_pred_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWUGjLLSMxvK"
      },
      "source": [
        "Normalizing the probabilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yljOTau5j6w0"
      },
      "source": [
        "prediction_proba = prediction_proba.div(prediction_proba.sum(axis=1), axis=0)\n",
        "prediction_proba"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPBdom3GVjr5"
      },
      "source": [
        "Submission File Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lIov-UeTR6r"
      },
      "source": [
        "#df_submission=pd.DataFrame(columns=['ID X PCODE', 'Label'])\n",
        "data_submission = []\n",
        "start_idx = 0\n",
        "for product in products:\n",
        "  for row in range(len(prediction_data.index)):\n",
        "    #df_submission['ID X PCODE'][start_idx]= df_test['ID'][row] + ' X ' + product\n",
        "    product_ID = df_test['ID'][row] + ' X ' + product\n",
        "    if df_test[product][row] != 1:      \n",
        "      #prob_1 = product + '_1'\n",
        "      prediction_value = prediction_proba[product][row]     \n",
        "    else:\n",
        "      prediction_value = 1      \n",
        "    data_submission.append([product_ID, prediction_value])\n",
        "    #start_idx += 1\n",
        "df_submission = pd.DataFrame(data_submission, columns=['ID X PCODE', 'Label'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7rb8CIcmU8K"
      },
      "source": [
        "df_submission.to_csv('submission.csv', index=False)\n",
        "from google.colab import files\n",
        "files.download(\"submission.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}